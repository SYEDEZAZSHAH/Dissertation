{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f629f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def get_posts(subreddit, before, after, size):\n",
    "    url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size={size}&before={before}&after={after}\"\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "# Scrape 2000 legal question titles\n",
    "legal_questions = []\n",
    "size = 100\n",
    "after = \"7d\" # posts from the last 7 days\n",
    "before = \"\" # no upper time limit\n",
    "subreddit = \"legaladvice\"\n",
    "while len(legal_questions) < 2000: # scrape until we have 2000 legal questions\n",
    "    data = get_posts(subreddit, before, after, size)\n",
    "    if not data: # if data is empty, break out of loop\n",
    "        break\n",
    "    for post in data:\n",
    "        if post[\"title\"][-1] == \"?\" and len(legal_questions) < 2000:\n",
    "            legal_questions.append({\"title\": post[\"title\"], \"label\": 1})\n",
    "        if len(legal_questions) >= 2000: # break out of loop if we have 2000 legal questions\n",
    "            break\n",
    "    before = data[-1][\"created_utc\"] # set before time for next page\n",
    "\n",
    "# Scrape 2000 non-legal question titles\n",
    "non_legal_questions = []\n",
    "size = 100\n",
    "after = \"7d\" # posts from the last 7 days\n",
    "before = \"\" # no upper time limit\n",
    "subreddit = \"askreddit\"\n",
    "keywords = [\"legal\", \"law\", \"court\"]\n",
    "while len(non_legal_questions) < 2000: # scrape until we have 2000 non-legal questions\n",
    "    data = get_posts(subreddit, before, after, size)\n",
    "    if not data: # if data is empty, break out of loop\n",
    "        break\n",
    "    for post in data:\n",
    "        title = post[\"title\"]\n",
    "        if not any(keyword in title.lower() for keyword in keywords) and title[-1] == \"?\" and len(non_legal_questions) < 2000:\n",
    "            non_legal_questions.append({\"title\": title, \"label\": 0})\n",
    "            if len(non_legal_questions) >= 2000: # break out of loop if we have 2000 non-legal questions\n",
    "                break\n",
    "    before = data[-1][\"created_utc\"] # set before time for next page\n",
    "\n",
    "# Combine legal and non-legal questions into a single list and shuffle the order\n",
    "questions = legal_questions + non_legal_questions\n",
    "random.shuffle(questions)\n",
    "\n",
    "# Convert list of questions to a dataframe and save to a CSV file\n",
    "df = pd.DataFrame(questions)\n",
    "df.to_csv(\"questions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c56a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/syedezazshah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/syedezazshah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3149 classified questions to classified_questions.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# function to classify legal questions based on serenity\n",
    "def classify_serenity(question):\n",
    "    # tokenize the question\n",
    "    tokens = word_tokenize(question)\n",
    "    # remove stopwords\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    # check if question contains keywords indicating a high-serenity issue\n",
    "    if any(keyword in tokens for keyword in [\"emergency\", \"urgent\", \"serious\", \"severe\", \"imminent\", \"dangerous\"]):\n",
    "        return 4  # high-serenity\n",
    "    # check if question contains keywords indicating a medium-serenity issue\n",
    "    elif any(keyword in tokens for keyword in [\"important\", \"concerned\", \"need\", \"worried\", \"issue\"]):\n",
    "        return 3  # medium-serenity\n",
    "    # check if question contains keywords indicating a low-serenity issue\n",
    "    elif any(keyword in tokens for keyword in [\"minor\", \"trivial\", \"small\", \"insignificant\", \"nuisance\"]):\n",
    "        return 2  # low-serenity\n",
    "    # default: 0 serenity\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# read questions from CSV file\n",
    "df = pd.read_csv(\"questions.csv\")\n",
    "\n",
    "# classify legal questions based on serenity\n",
    "df.loc[:, 'serenity'] = df['title'].apply(classify_serenity)\n",
    "\n",
    "# set serenity value to 0 for non-legal questions\n",
    "df.loc[df['label'] == 0, 'serenity'] = 0\n",
    "\n",
    "# drop label column\n",
    "df = df.drop(columns=['label'])\n",
    "\n",
    "# save classified questions to CSV file\n",
    "df.to_csv(\"classified_questions.csv\", index=False)\n",
    "\n",
    "print(\"Saved\", len(df), \"classified questions to classified_questions.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fd5c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your legal question: I have a legal issue with my landlord?\n",
      "Predicted serenity: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# load data from legal_questions.csv\n",
    "df = pd.read_csv(\"classified_questions.csv\")\n",
    "\n",
    "# prepare data for training\n",
    "X = df['title'].values\n",
    "y = df['serenity'].values\n",
    "\n",
    "# vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# train a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# predict the serenity of a new question\n",
    "new_question = input(\"Enter your legal question: \")\n",
    "new_question_vec = vectorizer.transform([new_question])\n",
    "predicted_serenity = clf.predict(new_question_vec)[0]\n",
    "print(\"Predicted serenity:\", predicted_serenity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b86f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
